name: Destroy Infrastructure

on:
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment to destroy"
        required: true
        default: "development"
        type: choice
        options:
          - development
          - production
      confirm:
        description: 'Type "destroy" to confirm'
        required: true
        type: string
      destroy_state_storage:
        description: "Also destroy Terraform state storage (S3 + DynamoDB)?"
        required: false
        default: false
        type: boolean

env:
  TERRAFORM_VERSION: "1.6.0"

jobs:
  destroy:
    name: Destroy Infrastructure
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment }}

    permissions:
      id-token: write
      contents: read

    steps:
      - name: Validate confirmation
        run: |
          if [ "${{ github.event.inputs.confirm }}" != "destroy" ]; then
            echo "❌ Confirmation failed. You must type 'destroy' to proceed."
            exit 1
          fi
          echo "✅ Confirmation validated"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Create dummy build directories
        run: |
          # Create minimal directory structure to satisfy Terraform data sources
          mkdir -p dist/create-todo dist/read-todos dist/update-todo
          echo '{}' > dist/create-todo/index.js
          echo '{}' > dist/read-todos/index.js  
          echo '{}' > dist/update-todo/index.js

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ github.event.inputs.environment == 'production' && secrets.AWS_ROLE_ARN_PROD || secrets.AWS_ROLE_ARN_DEV }}
          aws-region: ${{ vars.AWS_REGION || 'eu-west-2' }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Setup remote state backend
        working-directory: terraform
        run: |
          # Create backend configuration for the selected environment
          cat > backend-config.hcl << EOF
          bucket  = "terraform-state-serverless-todo-api"
          key     = "environments/${{ github.event.inputs.environment }}/terraform.tfstate"
          region  = "${{ vars.AWS_REGION || 'eu-west-2' }}"
          encrypt = true
          EOF

      - name: Terraform Init
        working-directory: terraform
        run: terraform init -backend-config=backend-config.hcl

      - name: Import existing resources into state
        working-directory: terraform
        run: |
          echo "🔍 Importing existing resources for ${{ github.event.inputs.environment }}..."

          # Set environment variable for resource naming
          ENV="${{ github.event.inputs.environment }}"

          # Try to import resources (ignore errors if they don't exist)
          set +e

          # Import IAM resources first (they're dependencies)
          terraform import aws_iam_role.lambda_execution_role "${ENV}-lambda-execution-role" || echo "IAM role not found"
          terraform import aws_iam_policy.lambda_dynamodb_policy "arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):policy/${ENV}-lambda-dynamodb-policy" || echo "IAM policy not found"
          terraform import aws_iam_role_policy_attachment.lambda_basic_execution "${ENV}-lambda-execution-role/arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole" || echo "Basic execution policy attachment not found"

          # Get the policy ARN for the attachment
          POLICY_ARN="arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):policy/${ENV}-lambda-dynamodb-policy"
          terraform import aws_iam_role_policy_attachment.lambda_dynamodb_policy_attachment "${ENV}-lambda-execution-role/${POLICY_ARN}" || echo "DynamoDB policy attachment not found"

          # Import Cognito IAM policy and attachment
          COGNITO_POLICY_ARN="arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):policy/${ENV}-lambda-cognito-policy"
          terraform import aws_iam_policy.lambda_cognito_policy "$COGNITO_POLICY_ARN" || echo "Cognito IAM policy not found"
          terraform import aws_iam_role_policy_attachment.lambda_cognito_policy_attachment "${ENV}-lambda-execution-role/${COGNITO_POLICY_ARN}" || echo "Cognito policy attachment not found"

          # Import DynamoDB table
          terraform import aws_dynamodb_table.todos "${ENV}-todos" || echo "todos table not found"

          # Import Cognito resources
          terraform import aws_cognito_user_pool.main "${ENV}-todo-user-pool" || echo "Cognito user pool not found"

          # Get User Pool ID for client import
          USER_POOL_ID=$(aws cognito-idp list-user-pools --max-items 50 --query "UserPools[?Name=='${ENV}-todo-user-pool'].Id" --output text)
          if [ ! -z "$USER_POOL_ID" ] && [ "$USER_POOL_ID" != "None" ] && [ "$USER_POOL_ID" != "null" ]; then
            echo "Found User Pool: $USER_POOL_ID"
            # Import User Pool Client
            CLIENT_ID=$(aws cognito-idp list-user-pool-clients --user-pool-id "$USER_POOL_ID" --query "UserPoolClients[?ClientName=='${ENV}-todo-app-client'].ClientId" --output text)
            if [ ! -z "$CLIENT_ID" ] && [ "$CLIENT_ID" != "None" ] && [ "$CLIENT_ID" != "null" ]; then
              terraform import aws_cognito_user_pool_client.main "${USER_POOL_ID}/${CLIENT_ID}" || echo "Cognito user pool client import failed"
            fi
          fi

          # Import Lambda functions
          terraform import aws_lambda_function.create_todo "${ENV}-create-todo" || echo "create-todo function not found"
          terraform import aws_lambda_function.read_todos "${ENV}-read-todos" || echo "read-todos function not found"
          terraform import aws_lambda_function.update_todo "${ENV}-update-todo" || echo "update-todo function not found"

          # Import CloudWatch Log Groups
          terraform import aws_cloudwatch_log_group.create_todo_logs "/aws/lambda/${ENV}-create-todo" || echo "create-todo logs not found"
          terraform import aws_cloudwatch_log_group.read_todos_logs "/aws/lambda/${ENV}-read-todos" || echo "read-todos logs not found"
          terraform import aws_cloudwatch_log_group.update_todo_logs "/aws/lambda/${ENV}-update-todo" || echo "update-todo logs not found"

          # Import API Gateway resources
          API_ID=$(aws apigateway get-rest-apis --query "items[?name=='${ENV}-todos-api'].id" --output text)
          if [ ! -z "$API_ID" ] && [ "$API_ID" != "None" ] && [ "$API_ID" != "null" ]; then
            echo "Found API Gateway: $API_ID"
            terraform import aws_api_gateway_rest_api.todos_api "$API_ID" || echo "API Gateway import failed"
            
            # Import Cognito Authorizer
            AUTHORIZER_ID=$(aws apigateway get-authorizers --rest-api-id "$API_ID" --query "items[?name=='${ENV}-cognito-authorizer'].id" --output text)
            if [ ! -z "$AUTHORIZER_ID" ] && [ "$AUTHORIZER_ID" != "None" ] && [ "$AUTHORIZER_ID" != "null" ]; then
              terraform import aws_api_gateway_authorizer.cognito_authorizer "$API_ID/$AUTHORIZER_ID" || echo "Cognito authorizer import failed"
            fi
            
            # Import API Gateway deployment and stage
            DEPLOYMENT_ID=$(aws apigateway get-deployments --rest-api-id "$API_ID" --query "items[0].id" --output text)
            if [ ! -z "$DEPLOYMENT_ID" ] && [ "$DEPLOYMENT_ID" != "None" ] && [ "$DEPLOYMENT_ID" != "null" ]; then
              terraform import aws_api_gateway_deployment.todos_api_deployment "$API_ID/$DEPLOYMENT_ID" || echo "API Gateway deployment import failed"
              terraform import aws_api_gateway_stage.todos_api_stage "$API_ID/${ENV}" || echo "API Gateway stage import failed"
            fi
          fi

          set -e
          echo "✅ Import completed"

      - name: Terraform Plan Destroy
        working-directory: terraform
        run: |
          terraform plan -destroy \
            -var="environment=${{ github.event.inputs.environment }}" \
            -var="aws_region=${{ vars.AWS_REGION || 'eu-west-2' }}" \
            -out=destroy-plan

      - name: Terraform Destroy
        working-directory: terraform
        run: terraform apply -auto-approve destroy-plan

      - name: Destroy State Storage (Optional)
        if: github.event.inputs.destroy_state_storage == 'true'
        run: |
          echo "🗑️ Destroying Terraform state storage for ${{ github.event.inputs.environment }}..."
          echo "⚠️  WARNING: This will delete the state file for ${{ github.event.inputs.environment }}"
          echo "⚠️  You will lose track of any remaining infrastructure in this environment"

          # Delete the specific environment's state file
          BUCKET_NAME="terraform-state-serverless-todo-api"
          STATE_KEY="environments/${{ github.event.inputs.environment }}/terraform.tfstate"

          echo "🗑️ Deleting state file: s3://$BUCKET_NAME/$STATE_KEY"
          aws s3 rm "s3://$BUCKET_NAME/$STATE_KEY" || echo "State file not found or already deleted"

          echo "✅ State storage cleanup completed for ${{ github.event.inputs.environment }}"
          echo "ℹ️  Note: S3 bucket is shared between environments and was not deleted"

      - name: Confirm destruction
        run: |
          echo "🗑️ Infrastructure for ${{ github.event.inputs.environment }} has been destroyed"
          if [ "${{ github.event.inputs.destroy_state_storage }}" == "true" ]; then
            echo "🗑️ Terraform state storage has also been destroyed"
          else
            echo "💾 Terraform state storage was preserved"
          fi
